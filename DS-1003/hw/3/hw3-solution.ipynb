{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\reals{\\mathbf{R}}$\n",
    "$\\newcommand\\cd{\\mathcal{D}}$\n",
    "$\\newcommand\\ch{\\mathcal{H}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculating Subgradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "Suppose $f_{1},\\ldots,f_{m}:\\reals^{d}\\to\\reals$\n",
    "are convex functions, and \n",
    "\\begin{align}\n",
    "f(x)=\\max_{i=1,\\ldots,,m}f_{i}(x).\n",
    "\\end{align}\n",
    "Let $k$ be any index for which $f_{k}(x)=f(x)$, and choose $g\\in\\partial f_{k}(x)$.\n",
    "Show that $g\\in\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof:\n",
    "Since $g \\in\\partial f_k(x)$, we have,\n",
    "\n",
    "\\begin{align}\n",
    "f_k(x+v) &\\geq f_k(x) + g^Tv &&\\forall v\\in\\reals^d\n",
    "\\end{align}\n",
    "\n",
    "But \n",
    "\\begin{align}\n",
    "f(x) &= \\max_{i=1,\\ldots,,m}f_{i}(x)\\\\\n",
    "&\\geq f_{i}(x) && \\forall i=1,\\ldots,,m\n",
    "\\end{align}\n",
    "\n",
    "Thus $\\forall y\\in \\reals^d$, we have $f(y) \\geq f_k(y)$\n",
    "\n",
    "Now observe that\n",
    "\\begin{align}\n",
    "f(x+v) &\\geq f_k(x+v)\\\\\n",
    "&\\geq f_k(x) + g^Tv\\\\\n",
    "&= f(x) + g^Tv && \\text{since $f(x)=f_k(x)$}\n",
    "\\end{align}\n",
    "\n",
    "Thus $g\\in\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "Give a subgradient of $J(w)=\\max\\left\\{ 0,1-yw^{T}x\\right\\} .$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2.1 we have\n",
    "\n",
    "\\begin{align}\n",
    "\\partial J(w) = \\begin{cases}\n",
    "-yx & yw^Tx \\leq 1\\\\\n",
    "0 & otherwise\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "Show that if $\\left\\{ x\\mid w^{T}x=0\\right\\} $ is a separating hyperplane\n",
    "for a training set $\\cd=\\left(\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\right)$,\n",
    "then the average perceptron loss on $\\cd$ is $0$. Thus any separating\n",
    "hyperplane of $\\cd$ is an empirical risk minimizer for perceptron\n",
    "loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "\n",
    "Assume $\\{x\\, \\lvert\\, w^Tx=0\\}$ is a separating hyperplane. Then $y_iw^Tx_i > 0\\,\\, \\forall\\,\\, i \\in \\{1, \\ldots, n\\}$.\n",
    "\n",
    "Now observe that\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Average perceptron loss} &= \\frac{1}{n} \\sum_{i=1}^n max\\{0, -\\hat{y_i}y_i\\}\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n max\\{0, -y_iw^Tx_i\\}\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n 0 && \\text{since $y_iw^Tx_i > 0$}\\\\\n",
    "&= 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "Let $\\ch$ be the linear hypothesis space consisting of functions\n",
    "$x\\mapsto w^{T}x$. Consider running stochastic subgradient descent\n",
    "(SSGD) to minimize the empirical risk with the perceptron loss. We'll\n",
    "use the version of SSGD in which we cycle through the data points\n",
    "in each epoch. Show that if we use a fixed step size $1$, we terminate\n",
    "when our training data are separated, and we make the right choice\n",
    "of subgradient,  then we are exactly doing the Perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By 2.1 we have $ \\partial l(\\hat{y},y) = \\begin{cases}\n",
    "-yx & yw^Tx < 0\\\\\n",
    "0 & otherwise\n",
    "\\end{cases}$\n",
    "\n",
    "With this choice of subgradient and a step size of 1 we can see that the update step is $w^{(k+1)} = \\begin{cases}\n",
    "w^{(k)} + y_ix_i & y_iw^Tx_i < 0\\\\\n",
    "w^{(k)} & otherwise\n",
    "\\end{cases}$.\n",
    "\n",
    "Note that this is precisely the update step in the perceptron algorithm.\n",
    "\n",
    "Now for the terminating conditions.\n",
    "\n",
    "Note that when the training data is separated, we have $y_iw^Tx_i > 0\\,\\, \\forall\\,\\, i \\in \\{1, \\ldots, n\\}$. In the perceptron algorithm this will result in all_correct = True and termination of that algorithm. Thus the terminating conditions are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "Suppose the perceptron algorithm returns $w$. Show that $w$ is a\n",
    "linear combination of the input points. That is, we can write $w=\\sum_{i=1}^{n}\\alpha_{i}x_{i}$\n",
    "for some $\\alpha_{1},\\ldots,\\alpha_{n}\\in\\reals$. The $x_{i}$ for\n",
    "which $\\alpha_{i}\\neq0$ are called support vectors. Give a characterization\n",
    "of points that are support vectors and not support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 3.2 we have that the update step is $w^{(k+1)} = \\begin{cases}\n",
    "w^{(k)} + y_ix_i & y_iw^Tx_i < 0\\\\\n",
    "w^{(k)} & otherwise\n",
    "\\end{cases}$.\n",
    "\n",
    "Then since $w^{(0)}=0$ and $y_i \\in \\{-1,1\\}$ it follows that can write $w=\\sum_{i=1}^{n}\\alpha_{i}x_{i}$\n",
    "for some $\\alpha_{1},\\ldots,\\alpha_{n}\\in\\reals$.\n",
    "\n",
    "The support vectors are vectors that were misclassified at some point during the optimization. When this happens, $y_iw^Tx_i < 0$ which results in $w^{(k+1)} = w^{(k)} + y_ix_i$ and so $\\alpha_{i}\\neq0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "Load all the data and randomly split it into 1500 training examples and 500 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import shuffle_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# I added a return statement to shuffle_data\n",
    "data = shuffle_data()\n",
    "train, val = train_test_split(data, train_size=1500, test_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1\n",
    "Write a function that converts an example (e.g. a list of words) into a sparse bag-of-words\n",
    "representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def to_sparse(word_list):\n",
    "    return Counter(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Support Vector Machine via Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1\n",
    "Consider the ``stochastic'' SVM objective function,\n",
    "which is the SVM objective function with a single training point\\footnote{Recall that if $i$ is selected uniformly from the set $\\left\\{ 1,\\ldots,m\\right\\} $,\n",
    "then this stochastic objective function has the same expected value\n",
    "as the full SVM objective function.}: $J_{i}(w)=\\frac{\\lambda}{2}\\|w\\|^{2}+\\max\\left\\{ 0,1-y_{i}w^{T}x_{i}\\right\\} $.\n",
    "The function $J_{i}(\\theta)$ is not differentiable everywhere. Give\n",
    "an expression for the gradient of $J_{i}(w)$ where it's defined,\n",
    "and specify where it is not defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "$\\nabla J_i(w) = \\begin{cases}\n",
    "\\lambda w -y_ix_i & y_iw^Tx_i < 1\\\\\n",
    "\\lambda w & y_iw^Tx_i > 1\\\\\n",
    "undefined & y_iw^Tx_i = 1\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 \n",
    "Show that a subgradient of $J_{i}(w)$ is given by \n",
    "\\begin{eqnarray*}\n",
    "g & = & \\begin{cases}\n",
    "\\lambda w-y_{i}x_{i} & \\mbox{for }y_{i}w^{T}x_{i}<1\\\\\n",
    "\\lambda w & \\mbox{for }y_{i}w^{T}x_{i}\\ge1.\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "You may use the following facts without proof: 1) If $f_{1},\\ldots,f_{m}:\\reals^{d}\\to\\reals$\n",
    "are convex functions and $f=f_{1}+\\cdots+f_{m}$, then $\\partial f(x)=\\partial f_{1}(x)+\\cdots+\\partial f_{m}(x)$.\n",
    "2) For $\\alpha\\ge0$, $\\partial\\left(\\alpha f\\right)(x)=\\alpha\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ f_1(w) = \\frac{\\lambda}{2} \\Vert w\\Vert^2$ and $ f_2(w) = max \\{0,1-y_iw^Tx_i\\}$. Note that $J_i(w) = f_1(w) + f_2(w)$.\n",
    "\n",
    "Now $\\partial f_1(w) = \\lambda w$.\n",
    "\n",
    "From the results of question 2.1 we also have $ \\partial f_2(w) = \\begin{cases}\n",
    "-y_{i}x_{i} & \\mbox{for }y_{i}w^{T}x_{i}<1\\\\\n",
    "0 &\\mbox{for }y_{i}w^{T}x_{i}\\ge1.\n",
    "\\end{cases}$\n",
    "\n",
    "Then from fact 1 we can conclude that $g = \\begin{cases}\n",
    "\\lambda w-y_{i}x_{i} & \\mbox{for }y_{i}w^{T}x_{i}<1\\\\\n",
    "\\lambda w & \\mbox{for }y_{i}w^{T}x_{i}\\ge1.\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
