{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\reals{\\mathbf{R}}$\n",
    "$\\newcommand\\cd{\\mathcal{D}}$\n",
    "$\\newcommand\\ch{\\mathcal{H}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculating Subgradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "Suppose $f_{1},\\ldots,f_{m}:\\reals^{d}\\to\\reals$\n",
    "are convex functions, and \n",
    "\\begin{align}\n",
    "f(x)=\\max_{i=1,\\ldots,,m}f_{i}(x).\n",
    "\\end{align}\n",
    "Let $k$ be any index for which $f_{k}(x)=f(x)$, and choose $g\\in\\partial f_{k}(x)$.\n",
    "Show that $g\\in\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof:\n",
    "Since $g \\in\\partial f_k(x)$, we have,\n",
    "\n",
    "\\begin{align}\n",
    "f_k(x+v) &\\geq f_k(x) + g^Tv &&\\forall v\\in\\reals^d\n",
    "\\end{align}\n",
    "\n",
    "But \n",
    "\\begin{align}\n",
    "f(x) &= \\max_{i=1,\\ldots,,m}f_{i}(x)\\\\\n",
    "&\\geq f_{i}(x) && \\forall i=1,\\ldots,,m\n",
    "\\end{align}\n",
    "\n",
    "Thus $\\forall y\\in \\reals^d$, we have $f(y) \\geq f_k(y)$\n",
    "\n",
    "Now observe that\n",
    "\\begin{align}\n",
    "f(x+v) &\\geq f_k(x+v)\\\\\n",
    "&\\geq f_k(x) + g^Tv\\\\\n",
    "&= f(x) + g^Tv && \\text{since $f(x)=f_k(x)$}\n",
    "\\end{align}\n",
    "\n",
    "Thus $g\\in\\partial f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "Give a subgradient of $J(w)=\\max\\left\\{ 0,1-yw^{T}x\\right\\} .$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2.1 we have\n",
    "\n",
    "\\begin{align}\n",
    "\\partial J(w) = \\begin{cases}\n",
    "-yx & yw^Tx \\leq 1\\\\\n",
    "0 & otherwise\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "Show that if $\\left\\{ x\\mid w^{T}x=0\\right\\} $ is a separating hyperplane\n",
    "for a training set $\\cd=\\left(\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\right)$,\n",
    "then the average perceptron loss on $\\cd$ is $0$. Thus any separating\n",
    "hyperplane of $\\cd$ is an empirical risk minimizer for perceptron\n",
    "loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "\n",
    "Assume $\\{x\\, \\lvert\\, w^Tx=0\\}$ is a separating hyperplane. Then $y_iw^Tx_i > 0\\,\\, \\forall\\,\\, i \\in \\{1, \\ldots, n\\}$.\n",
    "\n",
    "Now observe that\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Average perceptron loss} &= \\frac{1}{n} \\sum_{i=1}^n max\\{0, -\\hat{y_i}y_i\\}\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n max\\{0, -y_iw^Tx_i\\}\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n 0 && \\text{since $y_iw^Tx_i > 0$}\\\\\n",
    "&= 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "Let $\\ch$ be the linear hypothesis space consisting of functions\n",
    "$x\\mapsto w^{T}x$. Consider running stochastic subgradient descent\n",
    "(SSGD) to minimize the empirical risk with the perceptron loss. We'll\n",
    "use the version of SSGD in which we cycle through the data points\n",
    "in each epoch. Show that if we use a fixed step size $1$, we terminate\n",
    "when our training data are separated, and we make the right choice\n",
    "of subgradient,  then we are exactly doing the Perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By 2.1 we have $ \\partial l(\\hat{y},y) = \\begin{cases}\n",
    "-yx & yw^Tx < 0\\\\\n",
    "0 & otherwise\n",
    "\\end{cases}$\n",
    "\n",
    "With this choice of subgradient and a step size of 1 we can see that the update step is $w^{(k+1)} = \\begin{cases}\n",
    "w^{(k)} + y_ix_i & yw^Tx < 0\\\\\n",
    "w^{(k)} & otherwise\n",
    "\\end{cases}$.\n",
    "\n",
    "Note that this is precisely the update step in the perceptron algorithm.\n",
    "\n",
    "Now for the terminating conditions.\n",
    "\n",
    "Note that when the training data is separated, we have $y_iw^Tx_i > 0\\,\\, \\forall\\,\\, i \\in \\{1, \\ldots, n\\}$. In the perceptron algorithm this will result in all_correct = True and termination of that algorithm. Thus the terminating conditions are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
